<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>23.3.1. Scrapy爬虫框架 &mdash; 运维开发修炼之路</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="23.3.2. MongoDB的常用操作与导出" href="02.MongoDB%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E4%B8%8E%E5%AF%BC%E5%87%BA.html" />
    <link rel="prev" title="23.3. Scrapy" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> 小健_Python_go_Devops
            <img src="../../../_static/python_go.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                2.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../Go/index.html">Go语言学习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Go_vs_Python/index.html">Go vs Python</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Python全栈系列</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../01.Python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/index.html">1. Python数据类型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02.Python%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%E8%AF%AD%E5%8F%A5/index.html">2. Python流程控制语句</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../03.Python%E5%87%BD%E6%95%B0/index.html">3. Python函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04.Python%E5%86%85%E5%BB%BA%E5%87%BD%E6%95%B0/index.html">4. Python内建函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../05.Python%E6%8E%A8%E5%AF%BC%E5%BC%8F%E5%AD%A6%E4%B9%A0/index.html">5. Python推导式学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../06.Python%E8%BF%AD%E4%BB%A3%E5%99%A8_%E7%94%9F%E6%88%90%E5%99%A8_%E8%A3%85%E9%A5%B0%E5%99%A8/index.html">6. Python生成器、迭代器、装饰器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../07.Python%E9%9D%A2%E5%AF%B9%E5%AF%B9%E8%B1%A1%E8%AE%BE%E8%AE%A1_OOP/index.html">7. Python面对对象设计_OOP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../08.Python%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/index.html">8. Python异常处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../09.Python%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/index.html">9. Python文件操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../10.Python%E4%B8%AD%E7%9A%84%E5%8C%85%E5%92%8C%E6%A8%A1%E5%9D%97/index.html">10. Python中包和模块</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../11.Python%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/index.html">11. Python正则表达式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../12.Python%E6%A0%87%E5%87%86%E5%BA%93/index.html">12. Python 标准库学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../13.Python%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%BA%93/index.html">13. Python对数据库的操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../14.Python%E4%B8%89%E6%96%B9%E5%BA%93/index.html">14. Python 三方库学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../15.Python%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/index.html">15. Python 网络编程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../16.Python%E7%BA%BF%E7%A8%8B%E5%92%8C%E8%BF%9B%E7%A8%8B/index.html">16. Python 进程和线程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../17.Python%E8%AF%AD%E8%A8%80%E7%9A%84%E6%89%A9%E5%B1%95%E4%B8%8E%E5%B5%8C%E5%85%A5/index.html">17. Python与C语言扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../18.%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%E5%91%98%E7%9A%84Python%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">18. 系统管理员的Python脚本编程指南-读书笔记</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../20.Python%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/index.html">19. Python自动化运维最佳实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../21.Python%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/index.html">20. Python进阶学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../22.Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/index.html">21. Python网络爬虫</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../23.%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/index.html">22. 前端基础知识</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">23. Python框架</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../01.%E5%B8%B8%E7%94%A8%E7%9A%84GUI%E6%A1%86%E6%9E%B6/index.html">23.1. 常用的GUI框架</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02.Flask/index.html">23.2. Flask</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html">23.3. Scrapy</a><ul class="current">
<li class="toctree-l4 current"><a class="current reference internal" href="#">23.3.1. Scrapy爬虫框架</a></li>
<li class="toctree-l4"><a class="reference internal" href="02.MongoDB%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E4%B8%8E%E5%AF%BC%E5%87%BA.html">23.3.2. MongoDB的常用操作与导出</a></li>
<li class="toctree-l4"><a class="reference internal" href="03.Scrapy%E6%A1%86%E6%9E%B6%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0.html">23.3.3. Scrapy框架深入学习</a></li>
<li class="toctree-l4"><a class="reference internal" href="04.Scrapy%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%92%8C%E5%9B%BE%E7%89%87.html">23.3.4. Scrapy下载文件和图片</a></li>
<li class="toctree-l4"><a class="reference internal" href="05.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%EF%BC%9A%E7%88%AC%E5%8F%96matplotlib%E6%BA%90%E7%A0%81%E6%96%87%E4%BB%B6.html">23.3.5. 项目实战：爬取matplotlib源码文件</a></li>
<li class="toctree-l4"><a class="reference internal" href="06.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%EF%BC%9A%E4%B8%8B%E8%BD%BD360%E5%9B%BE%E7%89%87.html">23.3.6. 项目实战:下载360图片</a></li>
<li class="toctree-l4"><a class="reference internal" href="07.%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86webscraping.com%E7%BD%91%E7%AB%99.html">23.3.7. 模拟登陆webscraping.com网站</a></li>
<li class="toctree-l4"><a class="reference internal" href="08.%E8%AF%86%E5%88%AB%E9%AA%8C%E8%AF%81%E7%A0%81.html">23.3.8. 识别验证码</a></li>
<li class="toctree-l4"><a class="reference internal" href="09.Cookies%E7%99%BB%E5%BD%95.html">23.3.9. Cookies登录</a></li>
<li class="toctree-l4"><a class="reference internal" href="10.%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9ASQLite%E7%AF%87.html">23.3.10. 数据库：SQLite篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="11.%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9AMySQL%E7%AF%87.html">23.3.11. 数据库：MySQL篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="12.%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9AMongoDB%E7%AF%87.html">23.3.12. 数据库：MongoDB篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="13.%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9ARedis%E7%AF%87.html">23.3.13. 数据库：Redis篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="14.Fiddler%E5%AD%A6%E4%B9%A0.html">23.3.14. Fiddler学习</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../04.Django/index.html">23.4. Django</a></li>
<li class="toctree-l3"><a class="reference internal" href="../05.Tornado/index.html">23.5. Tornado</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06.Python%E9%AB%98%E6%95%88%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Django-Flask/index.html">23.6. Python高效开发实战-Django、Flask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07.Python-Django-Web%E5%85%B8%E5%9E%8B%E6%A8%A1%E5%9D%97%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/index.html">23.7. Python-Django-Web典型模块开发实战</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../25.Python%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/index.html">24. Python开发环境部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../26.%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E6%9C%AC%E7%AE%97%E6%B3%95%E4%B9%A6/index.html">25. 我的第一本算法书</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../27.Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/index.html">26. Python3网络爬虫开发实战</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../28.Python%E8%AE%A9%E7%B9%81%E7%90%90%E7%9A%84%E5%B7%A5%E4%BD%9C%E8%87%AA%E5%8A%A8%E5%8C%96/index.html">27. Python让繁琐的工作自动化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../29.%E7%96%AF%E7%8B%82%E7%9A%84Python%E8%AE%B2%E4%B9%89/index.html">28. 疯狂的Python讲义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../30.Django_Vue/index.html">29. Django_Vue</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../31.%E7%BC%96%E5%86%99Python%E7%9A%8490%E4%B8%AA%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/index.html">30. 编写Python的90个有效方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../32.Vue3.0%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/index.html">31. Vue3.0管理系统</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../Python%E6%B5%8B%E8%AF%95%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/index.html">Python测试开发入门与实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Vue/index.html">Vue.js企业开发实战</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Vue_Node.js/index.html">Vue.js+Node.js开发实战</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">小健_Python_go_Devops</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Python全栈系列</a> &raquo;</li>
          <li><a href="../index.html"><span class="section-number">23. </span>Python框架</a> &raquo;</li>
          <li><a href="index.html"><span class="section-number">23.3. </span>Scrapy</a> &raquo;</li>
      <li><span class="section-number">23.3.1. </span>Scrapy爬虫框架</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/Python/24.Python框架/03.Scrapy/01.Scrapy爬虫框架.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#scrapy" id="id8">Scrapy爬虫框架</a></p>
<ul>
<li><p><a class="reference internal" href="#id1" id="id9">Scrapy的安装</a></p></li>
<li><p><a class="reference internal" href="#id2" id="id10">明日学院里面给出的环境准备如下</a></p></li>
<li><p><a class="reference internal" href="#id3" id="id11">小猪短租网的信息</a></p>
<ul>
<li><p><a class="reference internal" href="#id4" id="id12">2.Scrapy文件介绍</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#id5" id="id13">开始爬取小猪租房网的信息</a></p></li>
<li><p><a class="reference internal" href="#id6" id="id14">Scrapy实战-爬取简书网热门专题信息</a></p></li>
<li><p><a class="reference internal" href="#scrapy-mongodb" id="id15">Scrapy实战 - 简书热门专题之MongoDB</a></p></li>
<li><p><a class="reference internal" href="#scrapy-mysql" id="id16">Scrapy实战 - 简书热门专题之MySQL</a></p></li>
<li><p><a class="reference internal" href="#id7" id="id17">Scrapy实战 - 简书推荐信息(Mongodb存储)</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="scrapy">
<h1><a class="toc-backref" href="#id8"><span class="section-number">23.3.1. </span>Scrapy爬虫框架</a><a class="headerlink" href="#scrapy" title="Permalink to this headline">¶</a></h1>
<section id="id1">
<h2><a class="toc-backref" href="#id9">Scrapy的安装</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>1.lxml库</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip3</span> <span class="n">install</span> <span class="n">lxml</span>
</pre></div>
</div>
<p>2.zope.interface库</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip3</span> <span class="n">install</span> <span class="n">zope</span><span class="o">.</span><span class="n">interface</span>
</pre></div>
</div>
<p>3.twisted库</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted

下载后的包为：Twisted-19.2.1-cp35-cp35m-win_amd64.whl   本人使用的是python3.5，请下载与本机系统相对于的版本

然后安装
pip3 install Twisted-19.2.1-cp35-cp35m-win_amd64.whl
</pre></div>
</div>
<img alt="../../../_images/Scrapy-Twisted002.png" src="../../../_images/Scrapy-Twisted002.png" />
<p>4.pyOpenSSL库安装</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip3</span> <span class="n">install</span> <span class="n">pyOpenSSL</span>
</pre></div>
</div>
<p>5.pywin32库</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/
下载后安装。
或者pip安装，pip安装失败就使用下载安装
pip3 install pywin32
</pre></div>
</div>
<p>安装后<code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">pywin32</span></code>会出错，就需要找到pywin32库，将其复制到
<code class="docutils literal notranslate"><span class="pre">C:/windows/system32</span></code>目录中 <img alt="image1" src="../../../_images/scrapy-base0001.png" /></p>
<p>6.Scrapy库</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip3</span> <span class="n">install</span> <span class="n">scrapy</span>
</pre></div>
</div>
</section>
<section id="id2">
<h2><a class="toc-backref" href="#id10">明日学院里面给出的环境准备如下</a><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># 搭建Scrapy爬虫框架

安装Twistedted模块
(1) 打开(https://www.lfd.uci.edu/~gohlke/pythonlibs/)
(2) 快捷键Ctr + F 搜索&quot;twisted&quot; 模块
(3) 下载&quot;twisted&quot; 模块的二进制文件（ 根据Python 版本和系统位数进行下载）
(4) 以管里员身份运行命令提示符窗口，
(5) 使cmd 命令打开&quot;Twisted&quot; 二进制文件所在的路径
(6) 通过pip 的方式进行安装

安装Scrapy
(1) pip install Scrapy
(2) 安装完成以后在命令行中输入&quot;scrapy&quot; 没有错误异常说明安装成功


安装pywin32模块
(1) 打开命令窗口， 然后输入pip install pywin32&quot; 命令，安装pywin32 模块。
(2) 安装完成以后， 在Python 命令行下输入&quot;import PYWin32 system32&quot; ， 如果没有提示惜误信息， 则表示安装成功。
</pre></div>
</div>
<p>参考文献</p>
<p><a class="reference external" href="https://blog.csdn.net/zwq912318834/article/details/77925220">https://blog.csdn.net/zwq912318834/article/details/77925220</a></p>
</section>
<section id="id3">
<h2><a class="toc-backref" href="#id11">小猪短租网的信息</a><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Scrapy爬虫框架：一个为了爬取网站信息，提取结构性数据而编写的应用爬虫框架，
该框架集数据字段定义、网络请求和解析、数据获取和处理等为一体，极大地方便了爬虫的编写过程。
</pre></div>
</div>
<p>①在CMD命令窗口中输入信息： <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">xiaozhu</span></code>
#建立爬虫项目，名为xiaozhu</p>
<img alt="../../../_images/scrapy0001.png" src="../../../_images/scrapy0001.png" />
<p>②通过pycharm查看xiaozhu项目下的所有文件： <img alt="image2" src="../../../_images/scrapy0002.png" /></p>
<p>③在spiders文件夹下新建xiaozhuspiders.py，用于编写爬虫代码：</p>
<section id="id4">
<h3><a class="toc-backref" href="#id12">2.Scrapy文件介绍</a><a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>如上图，最顶层的xiaozhu文件夹为项目名，第二层由与项目同名的文件夹xiaozhu和scrapy.cfg组成，其中xiaozhu文件夹就是模块，通常叫包，所有的爬虫代码都在这个包中添加，scrapy是这个项目的设置文件，其中的内容如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Automatically created by: scrapy startproject</span>
<span class="c1">#</span>
<span class="c1"># For more information about the [deploy] section see:</span>
<span class="c1"># https://scrapyd.readthedocs.io/en/latest/deploy.html</span>

<span class="p">[</span><span class="n">settings</span><span class="p">]</span>
<span class="n">default</span> <span class="o">=</span> <span class="n">xiaozhu</span><span class="o">.</span><span class="n">settings</span>

<span class="p">[</span><span class="n">deploy</span><span class="p">]</span>
<span class="c1">#url = http://localhost:6800/</span>
<span class="n">project</span> <span class="o">=</span> <span class="n">xiaozhu</span>
</pre></div>
</div>
<ul class="simple">
<li><p>除了注释部分，这个配置文件声明了两件事：</p></li>
</ul>
<p>①【settings】定义设置文件的位置为：xiaozhu模块下面的settings.py</p>
<p>②【deploy】定义项目名称为：xiaozhu</p>
<p>第三层由5个Python文件文件和spiders文件夹构成。spiders文件夹实际上也是一个模块。
而5个Python文件夹中，init.py是空文件，主要用于Python导入使用。</p>
<p>middlewares.py是spiders的中间件，主要负责对Request对象和Response对象进行处理，属于可选件。</p>
<p>-此处着重介绍其他3个Python文件items.py、pipelines.py、settings.py的使用：
①items.py文件：用于定义爬取的字段信息。自动生成的代码如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define here the models for your scraped items</span>
<span class="c1">#</span>
<span class="c1"># See documentation in:</span>
<span class="c1"># https://docs.scrapy.org/en/latest/topics/items.html</span>

<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">XiaozhuItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="c1"># define the fields for your item here like:</span>
    <span class="c1"># name = scrapy.Field()</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>②pipelines.py文件：用于爬虫数据的清洗和入库操作。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define your item pipelines here</span>
<span class="c1">#</span>
<span class="c1"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="c1"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>


<span class="k">class</span> <span class="nc">XiaozhuPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>③settings.py文件:用于对爬虫项目的一些设置，例如请求头的填写，设置pipelines.py爬取爬虫数据等。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Scrapy settings for xiaozhu project</span>
<span class="c1">#</span>
<span class="c1"># For simplicity, this file contains only settings considered important or</span>
<span class="c1"># commonly used. You can find more settings consulting the documentation:</span>
<span class="c1">#</span>
<span class="c1">#     https://docs.scrapy.org/en/latest/topics/settings.html</span>
<span class="c1">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span>
<span class="c1">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span>

<span class="n">BOT_NAME</span> <span class="o">=</span> <span class="s1">&#39;xiaozhu&#39;</span>

<span class="n">SPIDER_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;xiaozhu.spiders&#39;</span><span class="p">]</span>
<span class="n">NEWSPIDER_MODULE</span> <span class="o">=</span> <span class="s1">&#39;xiaozhu.spiders&#39;</span>


<span class="c1"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span>
<span class="c1">#USER_AGENT = &#39;xiaozhu (+http://www.yourdomain.com)&#39;</span>

<span class="c1"># Obey robots.txt rules</span>
<span class="n">ROBOTSTXT_OBEY</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span>
<span class="c1">#CONCURRENT_REQUESTS = 32</span>

<span class="c1"># Configure a delay for requests for the same website (default: 0)</span>
<span class="c1"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span>
<span class="c1"># See also autothrottle settings and docs</span>
<span class="c1">#DOWNLOAD_DELAY = 3</span>
<span class="c1"># The download delay setting will honor only one of:</span>
<span class="c1">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span>
<span class="c1">#CONCURRENT_REQUESTS_PER_IP = 16</span>

<span class="c1"># Disable cookies (enabled by default)</span>
<span class="c1">#COOKIES_ENABLED = False</span>

<span class="c1"># Disable Telnet Console (enabled by default)</span>
<span class="c1">#TELNETCONSOLE_ENABLED = False</span>

<span class="c1"># Override the default request headers:</span>
<span class="c1">#DEFAULT_REQUEST_HEADERS = {</span>
<span class="c1">#   &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,</span>
<span class="c1">#   &#39;Accept-Language&#39;: &#39;en&#39;,</span>
<span class="c1">#}</span>

<span class="c1"># Enable or disable spider middlewares</span>
<span class="c1"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span>
<span class="c1">#SPIDER_MIDDLEWARES = {</span>
<span class="c1">#    &#39;xiaozhu.middlewares.XiaozhuSpiderMiddleware&#39;: 543,</span>
<span class="c1">#}</span>

<span class="c1"># Enable or disable downloader middlewares</span>
<span class="c1"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span>
<span class="c1">#DOWNLOADER_MIDDLEWARES = {</span>
<span class="c1">#    &#39;xiaozhu.middlewares.XiaozhuDownloaderMiddleware&#39;: 543,</span>
<span class="c1">#}</span>

<span class="c1"># Enable or disable extensions</span>
<span class="c1"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span>
<span class="c1">#EXTENSIONS = {</span>
<span class="c1">#    &#39;scrapy.extensions.telnet.TelnetConsole&#39;: None,</span>
<span class="c1">#}</span>

<span class="c1"># Configure item pipelines</span>
<span class="c1"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
<span class="c1">#ITEM_PIPELINES = {</span>
<span class="c1">#    &#39;xiaozhu.pipelines.XiaozhuPipeline&#39;: 300,</span>
<span class="c1">#}</span>

<span class="c1"># Enable and configure the AutoThrottle extension (disabled by default)</span>
<span class="c1"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span>
<span class="c1">#AUTOTHROTTLE_ENABLED = True</span>
<span class="c1"># The initial download delay</span>
<span class="c1">#AUTOTHROTTLE_START_DELAY = 5</span>
<span class="c1"># The maximum download delay to be set in case of high latencies</span>
<span class="c1">#AUTOTHROTTLE_MAX_DELAY = 60</span>
<span class="c1"># The average number of requests Scrapy should be sending in parallel to</span>
<span class="c1"># each remote server</span>
<span class="c1">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span>
<span class="c1"># Enable showing throttling stats for every response received:</span>
<span class="c1">#AUTOTHROTTLE_DEBUG = False</span>

<span class="c1"># Enable and configure HTTP caching (disabled by default)</span>
<span class="c1"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span>
<span class="c1">#HTTPCACHE_ENABLED = True</span>
<span class="c1">#HTTPCACHE_EXPIRATION_SECS = 0</span>
<span class="c1">#HTTPCACHE_DIR = &#39;httpcache&#39;</span>
<span class="c1">#HTTPCACHE_IGNORE_HTTP_CODES = []</span>
<span class="c1">#HTTPCACHE_STORAGE = &#39;scrapy.extensions.httpcache.FilesystemCacheStorage&#39;</span>
</pre></div>
</div>
<p>第4层为spiders模块下的2个Python文件，init.py与前面所述一致，
xiaozhuspider.py是前面新建的Python文件，用于爬虫代码的编写。</p>
<p>综上所述，使用Scrapy爬虫框架相当于往里面填空，把scrapy项目中对应的文件代码补全即可实现爬虫。通常，我们设置以下4个文件即可完成爬虫任务。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>* items.py： 用于定义字段。
* xiaozhuspider.py：用于爬虫数据的获取。
* pipelines.py：用于爬虫数据的处理。
* settings.py：用于爬虫的设置。
</pre></div>
</div>
<p>3.下面爬取小组短租网的一页住房信息。
爬取字段为：标题、地址、价格、出租类型、居住人数和床位数。</p>
</section>
</section>
<section id="id5">
<h2><a class="toc-backref" href="#id13">开始爬取小猪租房网的信息</a><a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<img alt="../../../_images/scripy-xiaozhu0001.png" src="../../../_images/scripy-xiaozhu0001.png" />
<p>①items.py文件：文件开头的注释部分不需要修改，其他部分替换掉，即可完成爬虫的字段信息。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define here the models for your scraped items</span>
<span class="c1">#</span>
<span class="c1"># See documentation in:</span>
<span class="c1"># https://docs.scrapy.org/en/latest/topics/items.html</span>

<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">XiaozhuItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="c1"># define the fields for your item here like:</span>
    <span class="c1"># name = scrapy.Field()</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">address</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">lease_type</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">suggestion</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">bed</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>②xiaozhuspider.py文件：爬虫代码的编写。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding:utf8 -*-</span>
<span class="c1"># auther; 18793</span>
<span class="c1"># Date：2019/7/25 20:40</span>
<span class="c1"># filename: xiaozhuspider.py</span>
<span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span>  <span class="c1"># CrawlSpider作为xiaozhu的父类</span>
<span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>  <span class="c1"># Selector用于解析请求网页后返回的数据，与Lxml库的用法一样</span>
<span class="kn">from</span> <span class="nn">xiaozhu.items</span> <span class="kn">import</span> <span class="n">XiaozhuItem</span>  <span class="c1"># XiaozhuItem是items.py中定义爬虫字段的类</span>


<span class="k">class</span> <span class="nc">xiaozhu</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;xiaozhu&quot;</span>  <span class="c1"># 定义Spider的名字，在Scrapy爬虫运行时会用到：scrapy crawl xiaozhu</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://gz.xiaozhu.com/fangzi/112646821103.html&#39;</span><span class="p">]</span>  <span class="c1"># 默认情况下，spider会以start_urls中的链接为入口开始爬取</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>  <span class="c1"># response是请求网页返回的数据</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">XiaozhuItem</span><span class="p">()</span>  <span class="c1"># 初始化item</span>
        <span class="n">html</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>  <span class="c1"># 导入Selector用于解析数据</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">html</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&quot;pho_info&quot;]/h4/em/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># 唯一不同，需要加上.extract()来提取信息</span>
        <span class="n">address</span> <span class="o">=</span> <span class="n">html</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&quot;pho_info&quot;]/p/span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">price</span> <span class="o">=</span> <span class="n">html</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&quot;day_l&quot;]/span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">lease_type</span> <span class="o">=</span> <span class="n">html</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//li[@class=&quot;border_none&quot;]/h6/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">suggestion</span> <span class="o">=</span> <span class="n">html</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//h6[@class=&quot;h_ico2&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">bed</span> <span class="o">=</span> <span class="n">html</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//h6[@class=&quot;h_ico3&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">title</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">address</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">price</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;lease_type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lease_type</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;suggestion&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">suggestion</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;bed&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bed</span>

        <span class="k">yield</span> <span class="n">item</span>
</pre></div>
</div>
<p>③pipelines.py文件：将获取的item存入TXT文件中。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define your item pipelines here</span>
<span class="c1">#</span>
<span class="c1"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="c1"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>


<span class="k">class</span> <span class="nc">XiaozhuPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;xiaozhu.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;a+&quot;</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;address&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;lease_type&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;suggestion&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;bed&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>或者保存为<code class="docutils literal notranslate"><span class="pre">Json</span></code>格式文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define your item pipelines here</span>
<span class="c1">#</span>
<span class="c1"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="c1"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">codecs</span>


<span class="k">class</span> <span class="nc">XiaozhuPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span> <span class="o">=</span> <span class="n">codecs</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;mydata1.json&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>

    <span class="c1"># def process_item(self, item, spider):</span>
    <span class="c1">#     f = open(&quot;xiaozhu.txt&quot;, &quot;a+&quot;,encoding=&quot;utf-8&quot;)</span>
    <span class="c1">#     f.write(item[&#39;title&#39;] + &quot;\n&quot;)</span>
    <span class="c1">#     f.write(item[&#39;address&#39;] + &quot;\n&quot;)</span>
    <span class="c1">#     f.write(item[&#39;price&#39;] + &quot;\n&quot;)</span>
    <span class="c1">#     f.write(item[&#39;lease_type&#39;] + &quot;\n&quot;)</span>
    <span class="c1">#     f.write(item[&#39;suggestion&#39;] + &quot;\n&quot;)</span>
    <span class="c1">#     f.write(item[&#39;bed&#39;] + &quot;\n&quot;)</span>
    <span class="c1">#     return item</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="c1"># 通过json模块下的dumps()处理</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">),</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
<p>保存到<code class="docutils literal notranslate"><span class="pre">mydata1.json</span></code>中的数据为</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;price&quot;</span><span class="p">:</span> <span class="s2">&quot;308&quot;</span><span class="p">,</span>
    <span class="s2">&quot;address&quot;</span><span class="p">:</span> <span class="s2">&quot;广东省广州市海珠区广州大道南962号翠馨华庭&quot;</span><span class="p">,</span>
    <span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="s2">&quot;【广州塔A3】泳池TIT珠江新城地铁客村长隆&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lease_type&quot;</span><span class="p">:</span> <span class="s2">&quot;整套出租&quot;</span><span class="p">,</span>
    <span class="s2">&quot;suggestion&quot;</span><span class="p">:</span> <span class="s2">&quot;宜住3人&quot;</span><span class="p">,</span>
    <span class="s2">&quot;bed&quot;</span><span class="p">:</span> <span class="s2">&quot;共3张&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>④settings.py文件：在原有代码上恢复下述已经生成的代码，用于指定爬取的信息使用pipelines.py处理。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configure item pipelines</span>
<span class="c1"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
<span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s1">&#39;xiaozhu.pipelines.XiaozhuPipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>4.Scrapy爬虫的运行 在xiaozhu文件夹中，输入以下命令：</p>
<p><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">xiaozhu</span></code></p>
<img alt="../../../_images/scripy-xiaozhu00002.png" src="../../../_images/scripy-xiaozhu00002.png" />
<p>查看<code class="docutils literal notranslate"><span class="pre">xiaozhu.txt</span></code>文件,内容如下：</p>
<img alt="../../../_images/scripy-xiaozhu00003.png" src="../../../_images/scripy-xiaozhu00003.png" />
<p>当然，也可以在xiaozhu文件夹中新建一个main.py的文件，这样直接运行main.py即可运行爬虫程序。
代码如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding:utf8 -*-</span>
<span class="c1"># auther; 18793</span>
<span class="c1"># Date：2019/7/25 20:56</span>
<span class="c1"># filename: main.py</span>
<span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">cmdline</span>

<span class="n">cmdline</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;scrapy crawl xiaozhu&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>
</div>
<p>效果一样，如下所示: <img alt="image3" src="../../../_images/scrapy-xiaozhu0004.png" /></p>
</section>
<section id="id6">
<h2><a class="toc-backref" href="#id14">Scrapy实战-爬取简书网热门专题信息</a><a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>爬取网址：https://www.jianshu.com/recommendations/collections?order_by=hot
爬取内容：专题名称、专题介绍、收录文章、关注人数
爬取方式：Scrapy框架
储存方式：csv文件
</pre></div>
</div>
<p>使用F12，观察动态加载的URL，总共37页：</p>
<p>格式为</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>https://www.jianshu.com/recommendations/collections?page=1&amp;order_by=hot
https://www.jianshu.com/recommendations/collections?page=2&amp;order_by=hot
https://www.jianshu.com/recommendations/collections?page=3&amp;order_by=hot
https://www.jianshu.com/recommendations/collections?page=4&amp;order_by=hot
</pre></div>
</div>
<img alt="../../../_images/scrapy-jianshu0001.png" src="../../../_images/scrapy-jianshu0001.png" />
<p>①在CMD命令窗口中输入信息： <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">jianshu</span></code>
#建立爬虫项目，名为jianshu</p>
<p>1.items.py文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define here the models for your scraped items</span>
<span class="c1">#</span>
<span class="c1"># See documentation in:</span>
<span class="c1"># https://docs.scrapy.org/en/latest/topics/items.html</span>

<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">JianshuItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="c1"># define the fields for your item here like:</span>
    <span class="c1"># name = scrapy.Field()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">content</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">article</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">fans</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>2.Jianshuspider.py文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding:utf8 -*-</span>
<span class="c1"># auther; 18793</span>
<span class="c1"># Date：2019/7/25 21:08</span>
<span class="c1"># filename: Jianshuspider.py</span>
<span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span>
<span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="kn">from</span> <span class="nn">scrapy.http</span> <span class="kn">import</span> <span class="n">Request</span>
<span class="kn">from</span> <span class="nn">jianshu.items</span> <span class="kn">import</span> <span class="n">JianshuItem</span>


<span class="k">class</span> <span class="nc">Jianshu</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Jianshu&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://www.jianshu.com/recommendations/collections?page=1&amp;order_by=hot&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">JianshuItem</span><span class="p">()</span>
        <span class="n">selector</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&quot;collection-wrap&quot;]&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">infos</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">name</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/h4/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/p/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
                <span class="n">article</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div[@class=&quot;count&quot;]/a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">fans</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div[@class=&quot;count&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;· &#39;</span><span class="p">)</span>

                <span class="n">item</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span>
                <span class="n">item</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">content</span>
                <span class="n">item</span><span class="p">[</span><span class="s2">&quot;article&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">article</span>
                <span class="n">item</span><span class="p">[</span><span class="s2">&quot;fans&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fans</span>
                <span class="k">yield</span> <span class="n">item</span>

            <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
                <span class="k">pass</span>

        <span class="c1"># 构造第2页到第37页的‘热门专题’URL，通过Request请求URL，并回调parse()函数</span>
        <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://www.jianshu.com/recommendations/collections?page=</span><span class="si">{}</span><span class="s1">&amp;order_by=hot&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
                <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">37</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>3.settings.py文件： 此处使用Scrapy自带的存储功能（Feed
exports），所以不需要使用pipelines.py进行数据的处理存储。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span>
<span class="n">USER_AGENT</span> <span class="o">=</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36&#39;</span>
<span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># 休眠时间0.5s</span>

<span class="c1">#csv格式存储</span>
<span class="n">FEED_URI</span> <span class="o">=</span> <span class="s1">&#39;jianshu.csv&#39;</span>
<span class="n">FEEED_FORMAT</span> <span class="o">=</span> <span class="s2">&quot;csv&quot;</span>  <span class="c1"># 存入csv文件</span>
<span class="n">FEED_EXPORT_ENCODING</span> <span class="o">=</span> <span class="s2">&quot;gb18030&quot;</span>    <span class="c1">#设置编码</span>

<span class="c1">#json格式存储</span>
<span class="c1"># FEED_URI = &#39;jianshu.json&#39;</span>
<span class="c1"># FEEED_FORMAT = &quot;json&quot;  # 存入json文件</span>
<span class="c1"># FEED_EXPORT_ENCODING = &quot;utf-8&quot;        #设置中文编码</span>
</pre></div>
</div>
<p>4.main.py文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1">#-*- coding:utf8 -*-</span>
<span class="c1"># auther; 18793</span>
<span class="c1"># Date：2019/7/25 21:28</span>
<span class="c1"># filename: main.py</span>
<span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">cmdline</span>
<span class="n">cmdline</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;scrapy crawl Jianshu&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>
</div>
<p>运行main.py文件即可得到运行结果 <code class="docutils literal notranslate"><span class="pre">保存为csv格式</span></code> <img alt="image4" src="../../../_images/script-jianshu00005.png" />
如果用EXCEL文件打开，为乱码，或者setting中设置<code class="docutils literal notranslate"><span class="pre">FEED_EXPORT_ENCODING</span> <span class="pre">=</span> <span class="pre">&quot;gb18030&quot;</span></code>参数
需要先在Notepad+中选择编码“以UTF-8格式编码”,并重新保存即可。</p>
<p><code class="docutils literal notranslate"><span class="pre">保存为json格式</span></code> <img alt="image5" src="../../../_images/scrapy-jianshu-json001.png" /></p>
</section>
<section id="scrapy-mongodb">
<h2><a class="toc-backref" href="#id15">Scrapy实战 - 简书热门专题之MongoDB</a><a class="headerlink" href="#scrapy-mongodb" title="Permalink to this headline">¶</a></h2>
<p>在上一章基础上新建一个jianshu2的Scrapy项目，用MongoDB来存储，其他信息不变。</p>
<p>1.items.py不变</p>
<p>2.jianshu2spider.py：相关内容改成jianshu2。</p>
<p>3.pipelines.py内容如下</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define your item pipelines here</span>
<span class="c1">#</span>
<span class="c1"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="c1"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
<span class="kn">import</span> <span class="nn">pymongo</span>


<span class="k">class</span> <span class="nc">JianshuPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span> <span class="mi">27017</span><span class="p">)</span>
        <span class="n">mydb</span> <span class="o">=</span> <span class="n">client</span><span class="p">[</span><span class="s1">&#39;mydb&#39;</span><span class="p">]</span>
        <span class="n">jianshu</span> <span class="o">=</span> <span class="n">mydb</span><span class="p">[</span><span class="s1">&#39;jianshu&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">jianshu</span>  <span class="c1"># 连接数据库</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">info</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>  <span class="c1"># 插入数据库</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>4.setttings.py文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">USER_AGENT</span> <span class="o">=</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3294.6 Safari/537.36&#39;</span>     <span class="c1">#请求头</span>
<span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.5</span>                 <span class="c1">#睡眠时间0.5秒</span>
<span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s1">&#39;zhuanti2.pipelines.Zhuanti2Pipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>其他不变，运行<code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">Jianshu2</span></code>即可在MongoDB中得到结果。</p>
<p>或者</p>
<p>5.新建<code class="docutils literal notranslate"><span class="pre">main.py</span></code>文件作为程序的入口文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1">#-*- coding:utf8 -*-</span>
<span class="c1"># auther; 18793</span>
<span class="c1"># Date：2019/7/26 10:33</span>
<span class="c1"># filename: main.py</span>

<span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">cmdline</span>
<span class="n">cmdline</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;scrapy crawl Jianshu&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>
</div>
<img alt="../../../_images/scrapy-mongo0001.png" src="../../../_images/scrapy-mongo0001.png" />
</section>
<section id="scrapy-mysql">
<h2><a class="toc-backref" href="#id16">Scrapy实战 - 简书热门专题之MySQL</a><a class="headerlink" href="#scrapy-mysql" title="Permalink to this headline">¶</a></h2>
<p>在上一章基础上新建一个jianshu3的Scrapy项目，用MySQL来存储，其他信息不变。</p>
<p>1.items.py不变</p>
<p>2.jianshu3spider.py：相关内容改成jianshu3。</p>
<p>在SQLyog中建立数据表，然后选择“执行查询”命令进行代码的运行：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>CREATE TABLE `jianshu` (
  `id` int(10) NOT NULL AUTO_INCREMENT,
  `name` text,
  `content` text,
  `article` text,
  `fans` text,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
</pre></div>
</div>
<img alt="../../../_images/desc-scrapy-mysql0001.png" src="../../../_images/desc-scrapy-mysql0001.png" />
<p>4.pipelines.py内容如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define your item pipelines here</span>
<span class="c1">#</span>
<span class="c1"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="c1"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>

<span class="kn">import</span> <span class="nn">pymysql</span>


<span class="k">class</span> <span class="nc">Jianshu3Pipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">conn</span> <span class="o">=</span> <span class="n">pymysql</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span> <span class="n">user</span><span class="o">=</span><span class="s1">&#39;root&#39;</span><span class="p">,</span> <span class="n">passwd</span><span class="o">=</span><span class="s2">&quot;admin#123&quot;</span><span class="p">,</span>
                               <span class="n">db</span><span class="o">=</span><span class="s1">&#39;jianshu&#39;</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">3306</span><span class="p">,</span> <span class="n">charset</span><span class="o">=</span><span class="s1">&#39;utf8&#39;</span><span class="p">)</span>
        <span class="n">cursor</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">cursor</span>  <span class="c1"># 连接数据库</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">cursor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post</span>
        <span class="n">cursor</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;use jianshu&quot;</span><span class="p">)</span>  <span class="c1"># 选择数据表</span>
        <span class="n">sql</span> <span class="o">=</span> <span class="s2">&quot;INSERT INTO jianshu(name, content, article, fans) VALUES (</span><span class="si">%s</span><span class="s2">,</span><span class="si">%s</span><span class="s2">,</span><span class="si">%s</span><span class="s2">,</span><span class="si">%s</span><span class="s2">)&quot;</span>
        <span class="n">cursor</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">sql</span><span class="p">,</span> <span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;article&quot;</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;fans&quot;</span><span class="p">]))</span>
        <span class="n">cursor</span><span class="o">.</span><span class="n">connection</span><span class="o">.</span><span class="n">commit</span><span class="p">()</span>  <span class="c1"># 提交事务，将数据插入数据库</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>5.setttings.py文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">USER_AGENT</span> <span class="o">=</span> <span class="s2">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36&quot;</span>
<span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s1">&#39;jianshu3.pipelines.Jianshu3Pipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>6.其他不变.新建<code class="docutils literal notranslate"><span class="pre">main.py</span></code>文件，作为程序的入口。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1">#-*- coding:utf8 -*-</span>
<span class="c1"># auther; 18793</span>
<span class="c1"># Date：2019/7/26 10:33</span>
<span class="c1"># filename: main.py</span>
<span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">cmdline</span>
<span class="n">cmdline</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;scrapy crawl Jianshu3&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>
</div>
<p>7.查看Mysql数据库中数据的内容信息如下 <img alt="image6" src="../../../_images/scrapy-mysql0002.png" /></p>
</section>
<section id="id7">
<h2><a class="toc-backref" href="#id17">Scrapy实战 - 简书推荐信息(Mongodb存储)</a><a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>爬取网址：https://www.jianshu.com/recommendations/users
爬取内容：作者URL、最近更新文章；作者ID、“关注、粉丝、文章、字数、收获喜欢”
爬取方式：Scrapy框架
储存方式：mongodb
主要目的：实现跨页面爬虫
</pre></div>
</div>
<img alt="../../../_images/scrapy-pacong0001.png" src="../../../_images/scrapy-pacong0001.png" />
<img alt="../../../_images/scrapy-mongo000001.png" src="../../../_images/scrapy-mongo000001.png" />
<p>页面规律比较简单，通过F12查看XHR，即可获取翻页信息。</p>
<p>1.items.py文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define here the models for your scraped items</span>
<span class="c1">#</span>
<span class="c1"># See documentation in:</span>
<span class="c1"># https://docs.scrapy.org/en/latest/topics/items.html</span>

<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">JianshumongodbItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="c1"># define the fields for your item here like:</span>
    <span class="c1"># name = scrapy.Field()</span>
    <span class="n">author_url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">new_article</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">author_name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">focus</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">fans</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">article_num</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">write_num</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">like_num</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>2.JianshumongodbItem.py文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding:utf8 -*-</span>
<span class="c1"># auther; 18793</span>
<span class="c1"># Date：2019/7/26 11:01</span>
<span class="c1"># filename: Jianshumongospider.py</span>
<span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span>
<span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="kn">from</span> <span class="nn">scrapy.http</span> <span class="kn">import</span> <span class="n">Request</span>
<span class="kn">from</span> <span class="nn">jianshumongodb.items</span> <span class="kn">import</span> <span class="n">JianshumongodbItem</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">翻页Ajax异步加载的页面信息：</span>
<span class="sd">https://www.jianshu.com/recommendations/users?page=1</span>
<span class="sd">https://www.jianshu.com/recommendations/users?page=2</span>
<span class="sd">https://www.jianshu.com/recommendations/users?page=3</span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="k">class</span> <span class="nc">Jianshumongodb</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="c1"># 定义Spider的名字，在Scrapy爬虫运行时会用到：scrapy crawl jianshumongodb</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;jianshumongodb&quot;</span>

    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://www.jianshu.com/recommendations/users?page=1&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;https://www.jianshu.com/u/&quot;</span>
        <span class="n">selector</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&quot;col-xs-8&quot;]&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">infos</span><span class="p">:</span>
            <span class="n">author_url</span> <span class="o">=</span> <span class="n">base_url</span> <span class="o">+</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div/a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;/users/&#39;</span><span class="p">)</span>
            <span class="n">new_article</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div/div[@class=&quot;recent-update&quot;]&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">new_article</span> <span class="o">=</span> <span class="n">new_article</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;string(.)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>

            <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">author_url</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;author_url&#39;</span><span class="p">:</span> <span class="n">author_url</span><span class="p">,</span> <span class="s1">&#39;new_article&#39;</span><span class="p">:</span> <span class="n">new_article</span><span class="p">},</span>
                          <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_item</span><span class="p">)</span>
            <span class="c1"># 通过meta进行爬虫信息参数的传递，并通过Request请求author_url,回调parse_item()函数。</span>

        <span class="c1"># 制作翻页功能</span>
        <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://www.jianshu.com/recommendations/users?page=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">30</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>  <span class="c1"># 回调parse()函数</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>  <span class="c1"># 定义parse_item()爬取详细页面的信息</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">JianshumongodbItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;author_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s1">&#39;author_url&#39;</span><span class="p">]</span>  <span class="c1"># 取出传递的参数meta</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;new_article&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s1">&#39;new_article&#39;</span><span class="p">]</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">selector</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
            <span class="n">focus</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&quot;info&quot;]/ul/li[1]/div/a/p/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">fans</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&quot;info&quot;]/ul/li[2]/div/a/p/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">article_num</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&quot;info&quot;]/ul/li[3]/div/a/p/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">write_num</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&quot;info&quot;]/ul/li[4]/div/p/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">like_num</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&quot;info&quot;]/ul/li[5]/div/p/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;focus&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">focus</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;fans&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fans</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;article_num&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">article_num</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;write_num&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">write_num</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;like_num&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">like_num</span>
            <span class="k">yield</span> <span class="n">item</span>

        <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
            <span class="k">pass</span>




<span class="c1"># /html/body/div[1]/div/div[1]/div[1]/div[2]/</span>
<span class="c1"># /html/body/div[1]/div/div[1]/div[1]/div[2]/ul/li[2]/div/a/p</span>
<span class="c1"># /html/body/div[1]/div/div[1]/div[1]/div[2]/ul/li[3]/div/a/p</span>
<span class="c1"># 字数： /html/body/div[1]/div/div[1]/div[1]/div[2]/ul/li[4]/div/p</span>
<span class="c1"># like数：/html/body/div[1]/div/div[1]/div[1]/div[2]/ul/li[5]/div/p</span>
</pre></div>
</div>
<p>3.pipelines.py文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define your item pipelines here</span>
<span class="c1">#</span>
<span class="c1"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="c1"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>

<span class="kn">import</span> <span class="nn">pymongo</span>


<span class="k">class</span> <span class="nc">JianshumongodbPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span> <span class="mi">27017</span><span class="p">)</span>
        <span class="n">mydb</span> <span class="o">=</span> <span class="n">client</span><span class="p">[</span><span class="s1">&#39;mydb&#39;</span><span class="p">]</span>
        <span class="n">author</span> <span class="o">=</span> <span class="n">mydb</span><span class="p">[</span><span class="s1">&#39;author&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">author</span>  <span class="c1">##连接数据库</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">info</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>  <span class="c1">##插入数据库</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>4.settings.py文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">USER_AGENT</span> <span class="o">=</span> <span class="s2">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36&quot;</span>
<span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s1">&#39;jianshumongodb.pipelines.JianshumongodbPipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>5.新建<code class="docutils literal notranslate"><span class="pre">main.py</span></code>文件，作为程序的入口。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1">#-*- coding:utf8 -*-</span>
<span class="c1"># auther; 18793</span>
<span class="c1"># Date：2019/7/26 12:46</span>
<span class="c1"># filename: mian.py</span>

<span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">cmdline</span>
<span class="n">cmdline</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;scrapy crawl jianshumongodb&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>
</div>
<p>查看mongodb中存储的数据如下 <img alt="image7" src="../../../_images/scrapy-mongodb00001.png" /></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="23.3. Scrapy" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="02.MongoDB%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E4%B8%8E%E5%AF%BC%E5%87%BA.html" class="btn btn-neutral float-right" title="23.3.2. MongoDB的常用操作与导出" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, huxiaojian.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>