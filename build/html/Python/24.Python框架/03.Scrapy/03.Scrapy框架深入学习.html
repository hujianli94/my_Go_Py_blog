<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>23.3.3. Scrapy框架深入学习 &mdash; 运维开发修炼之路</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="23.3.4. Scrapy下载文件和图片" href="04.Scrapy%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%92%8C%E5%9B%BE%E7%89%87.html" />
    <link rel="prev" title="23.3.2. MongoDB的常用操作与导出" href="02.MongoDB%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E4%B8%8E%E5%AF%BC%E5%87%BA.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> 小健_Python_go_Devops
            <img src="../../../_static/python_go.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                2.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../Go/index.html">Go语言学习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Go_vs_Python/index.html">Go vs Python</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Python全栈系列</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../01.Python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/index.html">1. Python数据类型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02.Python%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%E8%AF%AD%E5%8F%A5/index.html">2. Python流程控制语句</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../03.Python%E5%87%BD%E6%95%B0/index.html">3. Python函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04.Python%E5%86%85%E5%BB%BA%E5%87%BD%E6%95%B0/index.html">4. Python内建函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../05.Python%E6%8E%A8%E5%AF%BC%E5%BC%8F%E5%AD%A6%E4%B9%A0/index.html">5. Python推导式学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../06.Python%E8%BF%AD%E4%BB%A3%E5%99%A8_%E7%94%9F%E6%88%90%E5%99%A8_%E8%A3%85%E9%A5%B0%E5%99%A8/index.html">6. Python生成器、迭代器、装饰器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../07.Python%E9%9D%A2%E5%AF%B9%E5%AF%B9%E8%B1%A1%E8%AE%BE%E8%AE%A1_OOP/index.html">7. Python面对对象设计_OOP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../08.Python%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/index.html">8. Python异常处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../09.Python%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/index.html">9. Python文件操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../10.Python%E4%B8%AD%E7%9A%84%E5%8C%85%E5%92%8C%E6%A8%A1%E5%9D%97/index.html">10. Python中包和模块</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../11.Python%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/index.html">11. Python正则表达式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../12.Python%E6%A0%87%E5%87%86%E5%BA%93/index.html">12. Python 标准库学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../13.Python%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%BA%93/index.html">13. Python对数据库的操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../14.Python%E4%B8%89%E6%96%B9%E5%BA%93/index.html">14. Python 三方库学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../15.Python%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/index.html">15. Python 网络编程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../16.Python%E7%BA%BF%E7%A8%8B%E5%92%8C%E8%BF%9B%E7%A8%8B/index.html">16. Python 进程和线程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../17.Python%E8%AF%AD%E8%A8%80%E7%9A%84%E6%89%A9%E5%B1%95%E4%B8%8E%E5%B5%8C%E5%85%A5/index.html">17. Python与C语言扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../18.%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%E5%91%98%E7%9A%84Python%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">18. 系统管理员的Python脚本编程指南-读书笔记</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../20.Python%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/index.html">19. Python自动化运维最佳实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../21.Python%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/index.html">20. Python进阶学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../22.Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/index.html">21. Python网络爬虫</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../23.%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/index.html">22. 前端基础知识</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">23. Python框架</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../01.%E5%B8%B8%E7%94%A8%E7%9A%84GUI%E6%A1%86%E6%9E%B6/index.html">23.1. 常用的GUI框架</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02.Flask/index.html">23.2. Flask</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html">23.3. Scrapy</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="01.Scrapy%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6.html">23.3.1. Scrapy爬虫框架</a></li>
<li class="toctree-l4"><a class="reference internal" href="02.MongoDB%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E4%B8%8E%E5%AF%BC%E5%87%BA.html">23.3.2. MongoDB的常用操作与导出</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">23.3.3. Scrapy框架深入学习</a></li>
<li class="toctree-l4"><a class="reference internal" href="04.Scrapy%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%92%8C%E5%9B%BE%E7%89%87.html">23.3.4. Scrapy下载文件和图片</a></li>
<li class="toctree-l4"><a class="reference internal" href="05.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%EF%BC%9A%E7%88%AC%E5%8F%96matplotlib%E6%BA%90%E7%A0%81%E6%96%87%E4%BB%B6.html">23.3.5. 项目实战：爬取matplotlib源码文件</a></li>
<li class="toctree-l4"><a class="reference internal" href="06.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%EF%BC%9A%E4%B8%8B%E8%BD%BD360%E5%9B%BE%E7%89%87.html">23.3.6. 项目实战:下载360图片</a></li>
<li class="toctree-l4"><a class="reference internal" href="07.%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86webscraping.com%E7%BD%91%E7%AB%99.html">23.3.7. 模拟登陆webscraping.com网站</a></li>
<li class="toctree-l4"><a class="reference internal" href="08.%E8%AF%86%E5%88%AB%E9%AA%8C%E8%AF%81%E7%A0%81.html">23.3.8. 识别验证码</a></li>
<li class="toctree-l4"><a class="reference internal" href="09.Cookies%E7%99%BB%E5%BD%95.html">23.3.9. Cookies登录</a></li>
<li class="toctree-l4"><a class="reference internal" href="10.%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9ASQLite%E7%AF%87.html">23.3.10. 数据库：SQLite篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="11.%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9AMySQL%E7%AF%87.html">23.3.11. 数据库：MySQL篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="12.%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9AMongoDB%E7%AF%87.html">23.3.12. 数据库：MongoDB篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="13.%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9ARedis%E7%AF%87.html">23.3.13. 数据库：Redis篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="14.Fiddler%E5%AD%A6%E4%B9%A0.html">23.3.14. Fiddler学习</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../04.Django/index.html">23.4. Django</a></li>
<li class="toctree-l3"><a class="reference internal" href="../05.Tornado/index.html">23.5. Tornado</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06.Python%E9%AB%98%E6%95%88%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Django-Flask/index.html">23.6. Python高效开发实战-Django、Flask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07.Python-Django-Web%E5%85%B8%E5%9E%8B%E6%A8%A1%E5%9D%97%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/index.html">23.7. Python-Django-Web典型模块开发实战</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../25.Python%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/index.html">24. Python开发环境部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../26.%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E6%9C%AC%E7%AE%97%E6%B3%95%E4%B9%A6/index.html">25. 我的第一本算法书</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../27.Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/index.html">26. Python3网络爬虫开发实战</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../28.Python%E8%AE%A9%E7%B9%81%E7%90%90%E7%9A%84%E5%B7%A5%E4%BD%9C%E8%87%AA%E5%8A%A8%E5%8C%96/index.html">27. Python让繁琐的工作自动化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../29.%E7%96%AF%E7%8B%82%E7%9A%84Python%E8%AE%B2%E4%B9%89/index.html">28. 疯狂的Python讲义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../30.Django_Vue/index.html">29. Django_Vue</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../31.%E7%BC%96%E5%86%99Python%E7%9A%8490%E4%B8%AA%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/index.html">30. 编写Python的90个有效方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../32.Vue3.0%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/index.html">31. Vue3.0管理系统</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../Python%E6%B5%8B%E8%AF%95%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/index.html">Python测试开发入门与实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Vue/index.html">Vue.js企业开发实战</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Vue_Node.js/index.html">Vue.js+Node.js开发实战</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">小健_Python_go_Devops</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Python全栈系列</a> &raquo;</li>
          <li><a href="../index.html"><span class="section-number">23. </span>Python框架</a> &raquo;</li>
          <li><a href="index.html"><span class="section-number">23.3. </span>Scrapy</a> &raquo;</li>
      <li><span class="section-number">23.3.3. </span>Scrapy框架深入学习</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/Python/24.Python框架/03.Scrapy/03.Scrapy框架深入学习.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#scrapy" id="id11">Scrapy框架深入学习</a></p>
<ul>
<li><p><a class="reference internal" href="#id1" id="id12">最简单的Scrapy爬虫程序：</a></p></li>
<li><p><a class="reference internal" href="#id2" id="id13">新闻供稿爬虫的Scrapy实现</a></p>
<ul>
<li><p><a class="reference internal" href="#id3" id="id14">构建模型</a></p></li>
<li><p><a class="reference internal" href="#id4" id="id15">编写爬虫程序</a></p></li>
<li><p><a class="reference internal" href="#crawlerprocess" id="id16">导入CrawlerProcess类</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#id5" id="id17">Scrapy常用函数及方法</a></p>
<ul>
<li><p><a class="reference internal" href="#id6" id="id18">2.常用方法</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#id7" id="id19">Scrapy项目调试</a></p></li>
<li><p><a class="reference internal" href="#cookie" id="id20">1.禁止Cookie</a></p></li>
<li><p><a class="reference internal" href="#id8" id="id21">2.设置下载延迟</a></p></li>
<li><p><a class="reference internal" href="#ip" id="id22">3.使用IP池</a></p></li>
<li><p><a class="reference internal" href="#id9" id="id23">4.使用用户代理池</a></p>
<ul>
<li><p><a class="reference internal" href="#id10" id="id24">注意！：</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<section id="scrapy">
<h1><a class="toc-backref" href="#id11"><span class="section-number">23.3.3. </span>Scrapy框架深入学习</a><a class="headerlink" href="#scrapy" title="Permalink to this headline">¶</a></h1>
<section id="id1">
<h2><a class="toc-backref" href="#id12">最简单的Scrapy爬虫程序：</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>①在CMD命令窗口中输入信息： <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">books</span></code> <img alt="image1" src="../../../_images/scrapy00001.png" /></p>
<p>注意： 通过<code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">books</span></code>
会输出很多日志信息，可以通过<code class="docutils literal notranslate"><span class="pre">--nolog</span></code>参数控制不显示日志信息</p>
<p><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">books</span> <span class="pre">--nolog</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">bookspiders.py</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding:utf8 -*-</span>
<span class="c1"># auther; 18793</span>
<span class="c1"># Date：2019/8/1 9:27</span>
<span class="c1"># filename: bookspiders.py</span>
<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">Books</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;books&#39;</span>  <span class="c1"># 建立唯一爬虫名，调用CMD命令时会用到</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://books.toscrape.com/&#39;</span><span class="p">]</span>  <span class="c1"># 爬取开始地址</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>  <span class="c1"># 默认解析函数</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//article&#39;</span><span class="p">)</span>  <span class="c1"># 直接使用response.xpath来解析信息</span>
        <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">infos</span><span class="p">:</span>
            <span class="n">title</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;h3/a/@title&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>  <span class="c1"># 最终提取信息时，加上.extract()</span>
            <span class="n">price</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div/p[@class=&quot;price_color&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>

            <span class="k">yield</span> <span class="p">{</span><span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">title</span><span class="p">,</span> <span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="n">price</span><span class="p">}</span>  <span class="c1"># 生成器返回数据信息</span>
</pre></div>
</div>
<p>运行命令 <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">books</span> <span class="pre">-o</span> <span class="pre">books.csv</span></code></p>
<p>查看结果如下 <img alt="image2" src="../../../_images/scrapy_base00001.png" /></p>
<p>当然，如果要爬取下一页的页面，共计50页，可以在parse()函数下添加以下代码，然后运行上述代码即可获取每页20条，</p>
<p>共计1000条的图书标题以及价格信息：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">next_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//li[@class=&quot;next&quot;]/a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">if</span> <span class="n">next_url</span><span class="p">:</span>
    <span class="n">next_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">next_url</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">next_url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id2">
<h2><a class="toc-backref" href="#id13">新闻供稿爬虫的Scrapy实现</a><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">startproject</span> <span class="n">chinanews_crawler</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ tree chinanews_crawler
chinanews_crawler
├── chinanews_crawler
│   ├── __init__.py      # 包定义
│   ├── __pycache__
│   ├── items.py         #模型定义
│   ├── middlewares.py
│   ├── pipelines.py     #管道定义
│   ├── settings.py      #配置文件
│   └── spiders          #蜘蛛文件夹
│       ├── __init__.py  #默认的蜘蛛代码文件
│       └── __pycache__
└── scrapy.cfg           # Scrapy的运行配置文件
</pre></div>
</div>
<p>快速生成蜘蛛命令工具：</p>
<p><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">genspider</span> <span class="pre">chinanews</span> <span class="pre">chanews.com</span></code></p>
<section id="id3">
<h3><a class="toc-backref" href="#id14">构建模型</a><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">items.py</span></code>
构建模型的方式编写一个<code class="docutils literal notranslate"><span class="pre">ChinanewsCrawlerItem</span></code>类，用于承载爬取回来的数据内容。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define here the models for your scraped items</span>
<span class="c1">#</span>
<span class="c1"># See documentation in:</span>
<span class="c1"># https://docs.scrapy.org/en/latest/topics/items.html</span>

<span class="kn">from</span> <span class="nn">scrapy.item</span> <span class="kn">import</span> <span class="n">Item</span><span class="p">,</span> <span class="n">Field</span>


<span class="k">class</span> <span class="nc">ChinanewsCrawlerItem</span><span class="p">(</span><span class="n">Item</span><span class="p">):</span>
    <span class="c1"># define the fields for your item here like:</span>
    <span class="c1"># name = scrapy.Field()</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>  <span class="c1"># 标题</span>
    <span class="n">link</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>  <span class="c1"># 新闻详情链接</span>
    <span class="n">desc</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>  <span class="c1"># 新闻综述</span>
    <span class="n">pub_date</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>  <span class="c1"># 发布日期</span>
</pre></div>
</div>
</section>
<section id="id4">
<h3><a class="toc-backref" href="#id15">编写爬虫程序</a><a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">chinanews.py</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">Spider</span>
<span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">from</span> <span class="nn">scrapy.http</span> <span class="kn">import</span> <span class="n">Request</span>
<span class="kn">from</span> <span class="nn">chinanews_crawler.items</span> <span class="kn">import</span> <span class="n">ChinanewsCrawlerItem</span>


<span class="k">class</span> <span class="nc">ChinanewsSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;chinanews&#39;</span>  <span class="c1"># 项目名称</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;chanews.com&#39;</span><span class="p">]</span>  <span class="c1"># 只爬取该域内的内容，自动过滤链接到其他域的内容,此项可以不声明</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;http://www.chinanews.com/rss/rss_2.html&#39;</span><span class="p">,)</span>  <span class="c1"># 蜘蛛被启动并产生第一批请求时的URL数组</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">rss_page</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">,</span> <span class="s1">&#39;html.parser&#39;</span><span class="p">)</span>
        <span class="n">rss_links</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;href&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">rss_page</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)])</span>

        <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">rss_links</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">link</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_feed</span><span class="p">,</span> <span class="n">dont_filter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>     <span class="c1">#忽略打开异常的网页，打不开的网页直接跳过</span>

    <span class="k">def</span> <span class="nf">parse_feed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">rss</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">,</span> <span class="s1">&#39;lxml&#39;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">rss</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;item&#39;</span><span class="p">):</span>
            <span class="n">feed_item</span> <span class="o">=</span> <span class="n">ChinanewsCrawlerItem</span><span class="p">()</span>
            <span class="n">feed_item</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">title</span><span class="o">.</span><span class="n">text</span><span class="p">,</span>
            <span class="n">feed_item</span><span class="p">[</span><span class="s1">&#39;link&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">link</span><span class="o">.</span><span class="n">text</span><span class="p">,</span>
            <span class="n">feed_item</span><span class="p">[</span><span class="s1">&#39;desc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">description</span><span class="o">.</span><span class="n">text</span><span class="p">,</span>
            <span class="n">feed_item</span><span class="p">[</span><span class="s1">&#39;pub_date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">pubdate</span><span class="o">.</span><span class="n">text</span>

            <span class="k">yield</span> <span class="n">feed_item</span>


<span class="c1"># 导入CrawlerProcess类</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerProcess</span>

<span class="c1"># 获取项目的设置信息</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.project</span> <span class="kn">import</span> <span class="n">get_project_settings</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># 创建CrawlerProcess类对象，并将获取的设置信息传入</span>
    <span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">(</span><span class="n">get_project_settings</span><span class="p">())</span>
    <span class="c1"># 设置需要启动的爬虫名称</span>
    <span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="s1">&#39;chinanews&#39;</span><span class="p">)</span>
    <span class="c1"># 启动爬虫</span>
    <span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
<p>设置数据输出的保存格式，可以利用Scrapy的配置让Scrapy默认输出到JSON文件，配置<code class="docutils literal notranslate"><span class="pre">setting.py</span></code>文件如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">USER_AGENT</span> <span class="o">=</span> <span class="s1">&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.54 Safari/536.5&#39;</span>
<span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># 休眠时间0.5s</span>

<span class="c1"># # csv格式存储</span>
<span class="c1"># FEED_URI = &#39;chinanews.csv&#39;</span>
<span class="c1"># FEEED_FORMAT = &quot;csv&quot;  # 存入csv文件</span>
<span class="c1"># FEED_EXPORT_ENCODING = &quot;gb18030&quot;    #设置编码</span>
<span class="c1"># # Obey robots.txt rules</span>

<span class="n">ROBOTSTXT_OBEY</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1">#json格式存储</span>
<span class="n">FEED_URI</span> <span class="o">=</span> <span class="s1">&#39;chinanews.json&#39;</span>
<span class="n">FEEED_FORMAT</span> <span class="o">=</span> <span class="s2">&quot;json&quot;</span>  <span class="c1"># 存入json文件</span>
<span class="n">FEED_EXPORT_ENCODING</span> <span class="o">=</span> <span class="s2">&quot;utf-8&quot;</span>        <span class="c1">#设置中文编码</span>
</pre></div>
</div>
<p>然后启动爬虫</p>
<p><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">chinanews</span></code> 输出结果如下： <img alt="image3" src="../../../_images/scrapy-json00002.png" /></p>
</section>
<section id="crawlerprocess">
<h3><a class="toc-backref" href="#id16">导入CrawlerProcess类</a><a class="headerlink" href="#crawlerprocess" title="Permalink to this headline">¶</a></h3>
<p>在文件<code class="docutils literal notranslate"><span class="pre">chinanews.py</span></code>中导入CrawlerProcess类，可以直接以主程序的方式运行爬虫蜘蛛程序。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 导入CrawlerProcess类</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerProcess</span>

<span class="c1"># 获取项目的设置信息</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.project</span> <span class="kn">import</span> <span class="n">get_project_settings</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># 创建CrawlerProcess类对象，并将获取的设置信息传入</span>
    <span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">(</span><span class="n">get_project_settings</span><span class="p">())</span>
    <span class="c1"># 设置需要启动的爬虫名称</span>
    <span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="s1">&#39;chinanews&#39;</span><span class="p">)</span>
    <span class="c1"># 启动爬虫</span>
    <span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="id5">
<h2><a class="toc-backref" href="#id17">Scrapy常用函数及方法</a><a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>1.spider开发流程：</p>
<p>最简单的Spider只需4个步骤：</p>
<p><code class="docutils literal notranslate"><span class="pre">1).继承scrapy.Spider；</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">2).为Spider取名；</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">3).设置爬取的起始点；</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">4).实现页面解析函数。</span></code></p>
<p>其中，Spider是一个基类，后面我们使用到的所有其他爬虫都需要继承这个Spider基类，</p>
<p>例如：CrawlSpider，XMLFeedSpider，CSVFeedSpider，SitemapSpider等，这些类全部位于scrapy:raw-latex:<cite>spiders目录之下</cite>。</p>
<p>实际上设置完爬取起始点后，默认由start_reqeusts()方法构建Request对象，然后默认指定由parse方法作为页面解析函数。如果我们希望为Request添加特定的请求头部或想为Request指定特定的页面解析函数，可以考虑在构建的Spider类中实现start_requests方法，即可覆盖基类Spider的start_requests方法。
例如，在第一章的基础上进行修改：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">Books</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;books&#39;</span>
    <span class="c1">#start_urls = [&#39;http://books.toscrape.com/&#39;]</span>

    <span class="c1">#实现start_requests方法，替代start_urls这个类属性</span>
    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://books.toscrape.com/&quot;</span><span class="p">,</span>
                             <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_book</span><span class="p">,</span>    <span class="c1">#此时改用parse_book作为回调函数</span>
                             <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span><span class="s1">&#39;Mozilla/5.0&#39;</span><span class="p">},</span>
                             <span class="n">dont_filter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_book</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">response</span><span class="p">):</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//article&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">infos</span><span class="p">:</span>
            <span class="n">title</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;h3/a/@title&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">price</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div/p[@class=&quot;price_color&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">yield</span> <span class="p">{</span><span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">title</span><span class="p">,</span> <span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="n">price</span><span class="p">}</span>
</pre></div>
</div>
<p>所以，设置爬取的起爬点有两种方法：</p>
<ul class="simple">
<li><p>定义start_urls属性</p></li>
<li><p>改写start_requests方法</p></li>
</ul>
<p>而第四个步骤，页面解析函数需要完成以下两个工作：</p>
<p>1).提取数据，将数据封装后（Item或字典）提交给Scrapy引擎；</p>
<p>2).提取链接，并用链接构造新的Request对象提交给Scrapy引擎；其中，提取链接的方法包括使用选择器或使用LinkExtractor。</p>
<section id="id6">
<h3><a class="toc-backref" href="#id18">2.常用方法</a><a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>1)提取常用方法</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>.extract() 对结果以列表的形式进行返回
.extract_first() 对extract()返回的结果列表取第一个元素。
.re() #对结果使用正则表达式进行再提取
.re_first() #返回第一个re()结果。
</pre></div>
</div>
<p>2)调用selector的方法
selector类的实现位于scrapy.selector模块，通过创建对象即可使用css或xpath解析方法。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>

<span class="k">class</span> <span class="nc">Book</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">response</span><span class="p">):</span>
        <span class="n">selector</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1&quot;</span><span class="p">)</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>当然，实际开发中，我们无需创建Selector对象，因为当我们第一次访问Response对象的selector属性时，
Response对象会自动创建Selector对象，同时在Response对象中内置了selector对象的css和xpath方法以供使用。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Book</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">response</span><span class="p">):</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>3)使用Item封装数据（items.py）
相对于使用字典来维护数据信息，使用item封装数据，有以下好处：</p>
<p>①清楚了解数据中包含哪些字段；</p>
<p>②包含对字段名字的检测；</p>
<p>③方便携带元数据，用于传递给其他组件的信息；</p>
<ul class="simple">
<li><p>数据段的基类：Item基类</p></li>
<li><p>描述数据包含哪些字段的类：FIeld类</p></li>
</ul>
<p>在items.py中这样写：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Item</span><span class="p">,</span><span class="n">Field</span>

<span class="k">class</span> <span class="nc">BooksItem</span><span class="p">(</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>在project为books，spiders文件夹下的books.py下这样写：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">books.items</span> <span class="kn">import</span> <span class="n">BooksItem</span>     <span class="c1">#引入items.py中创建的对象</span>

    <span class="k">def</span> <span class="nf">parse_book</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">response</span><span class="p">):</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//article&#39;</span><span class="p">)</span>
        <span class="n">book</span> <span class="o">=</span> <span class="n">BooksItem</span><span class="p">()</span>   <span class="c1">#实例化BooksItem()</span>

        <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">infos</span><span class="p">:</span>
            <span class="n">book</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;h3/a/@title&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">book</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div/p[@class=&quot;price_color&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">yield</span> <span class="n">book</span>      <span class="c1">#返回book</span>
</pre></div>
</div>
<p>4)使用Item Pipeline处理数据（pipelines.py） Item
Pipeline的几种典型应用：</p>
<ul class="simple">
<li><p>清洗数据</p></li>
<li><p>验证数据的有效性</p></li>
<li><p>过滤重复的数据</p></li>
<li><p>将数据存入数据库</p></li>
</ul>
<p>①Item
Pipeline不需要继承特定基类，只需要实现特定方法，例如：process_item、open_spider、close_spider。</p>
<p>②一个Item
Pipeline必须实现一个process_item(item,spider)方法，该方法用来处理每一项由Spider爬取到的数据，其中两个参数：
item: 爬取到的一项数据（Item或字典） spider:爬取此项数据的Spider对象</p>
<p><code class="docutils literal notranslate"><span class="pre">例如将Sharp</span> <span class="pre">Objects,£47.82中的英镑转换成人民币Sharp</span> <span class="pre">Objects,¥406.47。</span></code></p>
<p>代码为：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PriceConverterPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="n">exchange_rate</span> <span class="o">=</span> <span class="mf">8.5</span>  <span class="c1">#英镑对人民币汇率</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">price</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">:]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">exchange_rate</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">price</span>

        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>写入MongoDB的代码，方式一：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymongo</span>

<span class="k">class</span> <span class="nc">MongoDBPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span><span class="mi">27017</span><span class="p">)</span>
        <span class="n">test</span> <span class="o">=</span> <span class="n">client</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span>
        <span class="n">book</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;book&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">book</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">item</span><span class="p">,</span><span class="n">spider</span><span class="p">):</span>
        <span class="n">info</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>写入MongoDB的代码，方式二：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymongo</span>

<span class="k">class</span> <span class="nc">MongoDBPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">DB_URI</span> <span class="o">=</span> <span class="s1">&#39;mongodb://localhost:27017/&#39;</span>
    <span class="n">DB_NAME</span> <span class="o">=</span> <span class="s1">&#39;test&#39;</span>

    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">DB_URI</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">DB_NAME</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">collection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="p">[</span><span class="s1">&#39;book&#39;</span><span class="p">]</span>
        <span class="n">post</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">collection</span><span class="o">.</span><span class="n">insert_one</span><span class="p">(</span><span class="n">post</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>过滤重复数据，这里以书名作为主键判断重复项，实际上应该以ISBN编号为主键，只是前面仅爬取了书名和价格。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>

<span class="k">class</span> <span class="nc">DuplicatesPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">book_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">item</span><span class="p">,</span><span class="n">spider</span><span class="p">):</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">book_set</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s1">&#39;Duplicate book found:</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span><span class="n">item</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">book_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>由于Item Pipeline是可选的组件，想要启用某个Item
Pipeline，需要在<code class="docutils literal notranslate"><span class="pre">settings.py</span></code>中可对<code class="docutils literal notranslate"><span class="pre">Item</span></code>
<code class="docutils literal notranslate"><span class="pre">Pipeline</span></code>进行设置。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s1">&#39;books.pipelines.PriceConverterPipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
   <span class="s1">&#39;books.pipelines.MongoDBPipeline&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
   <span class="s1">&#39;books.pipelines.DuplicatesPipeline&#39;</span><span class="p">:</span> <span class="mi">400</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>其中，字典中的key为导入路径，后面的value是0~1000的数字。如果同时启动多个Pipeline，优先处理数字最小的Pipeline。</p>
<p>5)使用LinkExtractor提取链接
提取链接信息有两种方法，简单少量的链接使用Selector就足够了，而对于大量的链接或者复杂规则的链接，使用LinkExtractor更方便。
下面是代码的比较</p>
<ul class="simple">
<li><p>Selector()</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">next_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//li[@class=&quot;next&quot;]/a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">if</span> <span class="n">next_url</span><span class="p">:</span>
    <span class="n">next_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">next_url</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">next_url</span><span class="p">,</span><span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>LinkExtractor()</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>
<span class="nb">next</span> <span class="o">=</span> <span class="n">LinkExtractor</span><span class="p">(</span><span class="n">restrict_xpaths</span><span class="o">=</span><span class="s1">&#39;//li[@class=&quot;next&quot;]&#39;</span><span class="p">)</span>  <span class="c1">#LinkExtractor中添加限制条件，如果为空会提取页面的所有链接</span>
<span class="n">links</span> <span class="o">=</span> <span class="nb">next</span><span class="o">.</span><span class="n">extract_links</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>  <span class="c1">#返回一个Link对象的列表，里面包含链接。</span>

<span class="k">if</span> <span class="n">links</span><span class="p">:</span>
    <span class="n">next_url</span> <span class="o">=</span> <span class="n">links</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">url</span>   <span class="c1">#next_url[0]可获取Link对象，Link对象的url属性就是绝对地址，无需自己构建相对地址。</span>
    <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">next_url</span><span class="p">,</span><span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>6)使用Exporter导出数据（settings.py）</p>
<ul class="simple">
<li><p>可以使用命令行参数指定</p></li>
<li><p>通过配置文件指定</p></li>
</ul>
<p>命令行：</p>
<p><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">-o</span> <span class="pre">books.csv</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">-o</span> <span class="pre">books.csv</span> <span class="pre">-t</span> <span class="pre">csv</span></code> ## -t可以省略</p>
<p>配置文件：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>| 选项  |      含义      |  示例 |
|----------|:-------------:|------:|
| FEED_URI |  导出文件路径 | FEED_URI = &#39;books.csv&#39; |
| FEED_FORMAT|    导出数据格式   |   FEED_FORMAT = &#39;csv&#39; |
| FEED_EXPORT_ENCODING | 导出文件编码方式 |    FEED_EXPORT_ENCODING=&#39;gbk&#39; |
| FEED_EXPORT_FIELDS| 指定导出哪些字段并排序 |    FEED_EXPORT_FIELDS={&#39;title&#39;,&#39;price&#39;} |
| FEED_EXPORTERS| 用户自定义Exporter字典，一般用于添加新的导出数据格式 |    FEED_EXPORTERS ={‘excel’:&#39;项目名.新设置的py文件名.ExcelItemExporter&#39;} |
</pre></div>
</div>
</section>
</section>
<section id="id7">
<h2><a class="toc-backref" href="#id19">Scrapy项目调试</a><a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>爬取网址：http://books.toscrape.com/index.html
爬取信息：书名，价格，评价等级，产品编码，库存量，评价数量
爬取方式：scrapy框架
存储方式：csv文件
</pre></div>
</div>
<ol class="arabic simple">
<li><p>除了可以使用Chrome的开发者工具，还可以使用scrapy
shell命令，在交互式环境下调试。</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>cd 03.Scrapy调试
scrapy shell


# 获取请求信息
In [2]: fetch(&quot;http://books.toscrape.com/index.html&quot;)
2019-08-01 13:55:07 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://books.toscrape.com/index.html&gt; (referer: None)

* 也可以采用下列带URL的命令：
scrapy shell http://books.toscrape.com/index.html
</pre></div>
</div>
<ul class="simple">
<li><p>如果请求成功，可以看到： <img alt="image4" src="../../../_images/scrapy-shell000001.png" /></p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>使用view(response)查看respone包含的页面：</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">view(response)</span></code></p>
<ul class="simple">
<li><p>view函数是scrapy爬虫下载的页面，比直接打开浏览器下载的页面更靠谱，因为有时这两个页面是不同的，常规操作下有时还必须借助查看网页源代码来确认元素的位置。</p></li>
</ul>
<p>-由于response包含的页面也是用浏览器打开，接下来我们使用chrome进行元素审查。</p>
<img alt="../../../_images/scrapy_shell000003.png" src="../../../_images/scrapy_shell000003.png" />
<p>3.提取信息</p>
<p>1）由于我们需要获取的信息都在详细页面里面，我们需要先<code class="docutils literal notranslate"><span class="pre">提取链接</span></code>，可以使用<code class="docutils literal notranslate"><span class="pre">LinkExtractor</span></code>：</p>
<p>用三条语句就可以获取到链接信息了，而且使用LinkExtractor时不需要告诉链接的具体位置，只需要告诉链接所在的范围，非常方便。</p>
<p>此处提取信息，以下几条命令提取出来的信息是一样的，请看：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">le</span> <span class="o">=</span> <span class="n">LinkExtractor</span><span class="p">(</span><span class="n">restrict_css</span><span class="o">=</span><span class="s1">&#39;article.product_pod&#39;</span><span class="p">)</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LinkExtractor</span><span class="p">(</span><span class="n">restrict_css</span><span class="o">=</span><span class="s1">&#39;article.product_pod div a&#39;</span><span class="p">)</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LinkExtractor</span><span class="p">(</span><span class="n">restrict_xpaths</span><span class="o">=</span><span class="s1">&#39;//article[@class=&quot;product_pod&quot;]&#39;</span><span class="p">)</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LinkExtractor</span><span class="p">(</span><span class="n">restrict_xpaths</span><span class="o">=</span><span class="s1">&#39;//article[@class=&quot;product_pod&quot;]/div/a&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>2)获取下一页的链接地址</p>
<p>3)提取书的详细信息
使用fetch()命令，然后用view(response)即可看到请求页面。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">61</span><span class="p">]:</span> <span class="n">fetch</span><span class="p">(</span><span class="s2">&quot;http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html&quot;</span><span class="p">)</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">08</span><span class="o">-</span><span class="mi">01</span> <span class="mi">14</span><span class="p">:</span><span class="mi">49</span><span class="p">:</span><span class="mi">04</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">books</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">catalogue</span><span class="o">/</span><span class="n">a</span><span class="o">-</span><span class="n">light</span><span class="o">-</span><span class="ow">in</span><span class="o">-</span><span class="n">the</span><span class="o">-</span><span class="n">attic_1000</span><span class="o">/</span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">63</span><span class="p">]:</span> <span class="n">view</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">63</span><span class="p">]:</span> <span class="kc">True</span>
</pre></div>
</div>
<img alt="../../../_images/scrapy_view00001.png" src="../../../_images/scrapy_view00001.png" />
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">72</span><span class="p">]:</span> <span class="n">info</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//article/div[1]/div[2]/h1/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">73</span><span class="p">]:</span> <span class="n">info</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">73</span><span class="p">]:</span> <span class="s1">&#39;A Light in the Attic&#39;</span>


<span class="n">In</span> <span class="p">[</span><span class="mi">74</span><span class="p">]:</span> <span class="n">price</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//article/div[1]/div[2]/p[1]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">75</span><span class="p">]:</span> <span class="n">price</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">75</span><span class="p">]:</span> <span class="s1">&#39;£51.77&#39;</span>
</pre></div>
</div>
<p>当然，由于书名，价格，评价等级以及库存量均在一处，可以先找出大范围，然后再确定具体信息：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 先抓大再抓小</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">87</span><span class="p">]:</span> <span class="nb">all</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//*[@id=&quot;content_inner&quot;]/article&#39;</span><span class="p">)</span>


<span class="n">In</span> <span class="p">[</span><span class="mi">86</span><span class="p">]:</span> <span class="nb">all</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div[1]/div[2]/h1/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">86</span><span class="p">]:</span> <span class="s1">&#39;A Light in the Attic&#39;</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">91</span><span class="p">]:</span> <span class="n">price</span> <span class="o">=</span> <span class="nb">all</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div[1]/div[2]/p[1]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">92</span><span class="p">]:</span> <span class="n">price</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">92</span><span class="p">]:</span> <span class="s1">&#39;£51.77&#39;</span>


<span class="n">In</span> <span class="p">[</span><span class="mi">165</span><span class="p">]:</span> <span class="nb">all</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//table[@class=&quot;table table-striped&quot;]&#39;</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">166</span><span class="p">]:</span> <span class="nb">all</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s1">&#39;((\d+) available)&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">166</span><span class="p">]:</span> <span class="s1">&#39;22 available&#39;</span>


<span class="n">In</span> <span class="p">[</span><span class="mi">173</span><span class="p">]:</span> <span class="nb">all</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div[1]/div[2]/p[3]/@class&#39;</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">173</span><span class="p">]:</span> <span class="p">[</span><span class="o">&lt;</span><span class="n">Selector</span> <span class="n">xpath</span><span class="o">=</span><span class="s1">&#39;div[1]/div[2]/p[3]/@class&#39;</span> <span class="n">data</span><span class="o">=</span><span class="s1">&#39;star-rating Three&#39;</span><span class="o">&gt;</span><span class="p">]</span>


<span class="n">In</span> <span class="p">[</span><span class="mi">175</span><span class="p">]:</span> <span class="nb">all</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div[1]/div[2]/p[3]/@class&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">175</span><span class="p">]:</span> <span class="s1">&#39;star-rating Three&#39;</span>


<span class="n">In</span> <span class="p">[</span><span class="mi">178</span><span class="p">]:</span> <span class="nb">all</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div[1]/div[2]/p[3]/@class&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">178</span><span class="p">]:</span> <span class="s1">&#39;Three&#39;</span>
</pre></div>
</div>
<p>4)scrapy shell的退出：<code class="docutils literal notranslate"><span class="pre">exit()</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">exit</span><span class="p">()</span>
</pre></div>
</div>
<p>4.编码实现</p>
<p>1)创建项目</p>
<p><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">books</span></code></p>
<ul class="simple">
<li><p>新建项目<code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">books</span></code></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span>  <span class="n">books</span>
</pre></div>
</div>
<ul class="simple">
<li><p>利用模板生成spider文件<code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">genspider</span> <span class="pre">booksspider</span> <span class="pre">books.toscrape.com</span></code></p></li>
</ul>
<img alt="../../../_images/spider_books00001.png" src="../../../_images/spider_books00001.png" />
<p>上述生成的booksspider.py文件：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">BooksspiderSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;booksspider&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;books.toscrape.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://books.toscrape.com/&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span>
</pre></div>
</div>
<p>2)items.py</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">BooksItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>            <span class="c1">#书名</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>           <span class="c1">#价格</span>
    <span class="n">review_rating</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>   <span class="c1">#评价等级（1-5星）</span>
    <span class="n">review_num</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>      <span class="c1">#评价数量</span>
    <span class="n">upc</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>             <span class="c1">#产品编码</span>
    <span class="n">stock</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>           <span class="c1">#库存量</span>
</pre></div>
</div>
<p>3)booksspider.py</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">books.items</span> <span class="kn">import</span> <span class="n">BooksItem</span>
<span class="kn">from</span> <span class="nn">scrapy.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>


<span class="k">class</span> <span class="nc">BooksspiderSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;booksspider&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;books.toscrape.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://books.toscrape.com/&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1">##提取每本书的链接</span>
        <span class="n">le</span> <span class="o">=</span> <span class="n">LinkExtractor</span><span class="p">(</span><span class="n">restrict_xpaths</span><span class="o">=</span><span class="s1">&#39;//article[@class=&quot;product_pod&quot;]&#39;</span><span class="p">)</span>  <span class="c1">##具体位置在//article/div/a的标签中</span>
        <span class="n">detail_urls</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">extract_links</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">detail_url</span> <span class="ow">in</span> <span class="n">detail_urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">detail_url</span><span class="o">.</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_book</span><span class="p">)</span>  <span class="c1">##记得使用.url提取出extract_links里面的链接。</span>

        <span class="c1">##提取下一页的链接</span>
        <span class="n">le2</span> <span class="o">=</span> <span class="n">LinkExtractor</span><span class="p">(</span><span class="n">restrict_xpaths</span><span class="o">=</span><span class="s1">&#39;//li[@class=&quot;next&quot;]&#39;</span><span class="p">)</span>
        <span class="n">next_url</span> <span class="o">=</span> <span class="n">le2</span><span class="o">.</span><span class="n">extract_links</span><span class="p">(</span><span class="n">response</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">url</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">next_url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_book</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1">##提取每本书的具体信息</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">BooksItem</span><span class="p">()</span>
        <span class="n">info</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//article[@class=&quot;product_page&quot;]&#39;</span><span class="p">)</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div[1]/div[2]/h1/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div[1]/div[2]/p[1]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;review_rating&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;div[1]/div[2]/p[3]/@class&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s1">&#39;star-rating (\w+)&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">info2</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//table[@class=&quot;table table-striped&quot;]&#39;</span><span class="p">)</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;upc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">info2</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s1">&#39;&lt;td&gt;.*?&lt;/td&gt;&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&lt;td&gt;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&lt;/td&gt;&#39;</span><span class="p">)</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;stock&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">info2</span><span class="o">.</span><span class="n">re_first</span><span class="p">(</span><span class="s1">&#39;((\d+) available)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># item[&#39;stock&#39;] = info2.xpath(&#39;//tr[last()-1]/td/text()&#39;).re_first(&#39;\d+&#39;)  #使用last()获取标签的最后一个数字</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;review_num&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">info2</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s1">&#39;&lt;td&gt;.*?&lt;/td&gt;&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&lt;td&gt;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&lt;/td&gt;&#39;</span><span class="p">)</span>
        <span class="c1"># item[&#39;review_num&#39;] = info2.xpath(&#39;//tr[last()]/td/text()&#39;).extract_first()</span>
        <span class="k">yield</span> <span class="n">item</span>
</pre></div>
</div>
<p>运行<code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">booksspider</span> <span class="pre">-o</span> <span class="pre">books.csv</span></code>，结果为：</p>
<img alt="../../../_images/scrapy_debug0000003.png" src="../../../_images/scrapy_debug0000003.png" />
<p>4)改进点 ①<code class="docutils literal notranslate"><span class="pre">指定各列的次序</span></code> 在settings.py中加入以下代码：</p>
<p>②将评价等级中的One，Two，Three转变成1,2,3 在pipelines.py中加入以下代码：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Define your item pipelines here</span>
<span class="c1">#</span>
<span class="c1"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="c1"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>


<span class="k">class</span> <span class="nc">BooksPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">review_rating_map</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;One&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;Two&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;Three&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;Four&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="s2">&quot;Five&quot;</span><span class="p">:</span> <span class="mi">5</span>
    <span class="p">}</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="c1"># rating = item.get(&#39;review_rating&#39;)  #获取review_rating的数据</span>
        <span class="n">rating</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;review_rating&#39;</span><span class="p">]</span>  <span class="c1"># 与上面的语句等价</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;review_rating&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">review_rating_map</span><span class="p">[</span><span class="n">rating</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>在setttings.py中加入：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s1">&#39;books.pipelines.BooksPipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>结果为： <img alt="image5" src="../../../_images/scrapy-debug000002.png" /></p>
<p>修改为<code class="docutils literal notranslate"><span class="pre">main.py</span></code>文件的方式运行</p>
<p>main.py</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1">#-*- coding:utf8 -*-</span>
<span class="c1"># auther; 18793</span>
<span class="c1"># Date：2019/8/1 19:35</span>
<span class="c1"># filename: main.py</span>

<span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">cmdline</span>
<span class="n">cmdline</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;scrapy crawl booksspider -o books.csv&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>
</div>
<p>直接执行<code class="docutils literal notranslate"><span class="pre">main.py</span></code>文件即可运行程序。</p>
<p>[STRIKEOUT:修改<code class="docutils literal notranslate"><span class="pre">setting.py</span></code> 文件，修改存储格式为csv文件格式]</p>
<p>[STRIKEOUT:setting.py]</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">USER_AGENT</span> <span class="o">=</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36&#39;</span>
<span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># 休眠时间0.5s</span>

<span class="c1">#csv格式存储</span>
<span class="n">FEED_URI</span> <span class="o">=</span> <span class="s1">&#39;books.csv&#39;</span>
<span class="n">FEEED_FORMAT</span> <span class="o">=</span> <span class="s2">&quot;csv&quot;</span>  <span class="c1"># 存入csv文件</span>
<span class="n">FEED_EXPORT_ENCODING</span> <span class="o">=</span> <span class="s2">&quot;gb18030&quot;</span>    <span class="c1">#设置编码</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">setting.py</span></code> 文件 中的设置</p>
</section>
<section id="cookie">
<h2><a class="toc-backref" href="#id20">1.禁止Cookie</a><a class="headerlink" href="#cookie" title="Permalink to this headline">¶</a></h2>
<p>若要禁用Cookie的代码，只需要把<code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">COOKIES_ENABLED</span> <span class="pre">=</span> <span class="pre">False</span></code>
的注释去掉即可， 修改为 <code class="docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span> <span class="pre">=</span> <span class="pre">False</span></code>
这样就可以禁用本地的Cookie，让那些通过用户的Cookie信息对用户进行识别和分析的网站无法识别我们，即无法禁止我们爬取。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Disable cookies (enabled by default)</span>
<span class="c1"># COOKIES_ENABLED = False</span>

<span class="n">COOKIES_ENABLED</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</section>
<section id="id8">
<h2><a class="toc-backref" href="#id21">2.设置下载延迟</a><a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">设置爬虫下载网页的时间间隔为0.5秒，将对于的值改为0.5即可。</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># 休眠时间0.5s</span>
</pre></div>
</div>
</section>
<section id="ip">
<h2><a class="toc-backref" href="#id22">3.使用IP池</a><a class="headerlink" href="#ip" title="Permalink to this headline">¶</a></h2>
<p>直接编辑爬虫项目中的<code class="docutils literal notranslate"><span class="pre">setting.py</span></code>文件并添加如下信息：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#(2)</span>
<span class="c1">#IP池设置</span>
<span class="n">IPPOOL</span><span class="o">=</span><span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;ipaddr&quot;</span><span class="p">:</span><span class="s2">&quot;121.33.226.167:3128&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;ipaddr&quot;</span><span class="p">:</span><span class="s2">&quot;118.187.10.11:80&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;ipaddr&quot;</span><span class="p">:</span><span class="s2">&quot;123.56.245.138:808&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;ipaddr&quot;</span><span class="p">:</span><span class="s2">&quot;139.196.108.68:80&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;ipaddr&quot;</span><span class="p">:</span><span class="s2">&quot;36.250.87.88:47800&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;ipaddr&quot;</span><span class="p">:</span><span class="s2">&quot;123.57.190.51:7777&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;ipaddr&quot;</span><span class="p">:</span><span class="s2">&quot;171.39.26.176:8123&quot;</span><span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<p>IPPOOL就是对于的代理服务器的IP池，外层通过列表的形式存储，里层通过字典的形式存储</p>
<p>当网站对IP进行封禁的时候，我们可以使用代理IP。
在项目核心目录下建立一个下载中间件，创建文件<code class="docutils literal notranslate"><span class="pre">middlewares.py</span></code>，可以将该文件作为下载中间件文件使用。</p>
<p><code class="docutils literal notranslate"><span class="pre">middlewares.py</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#(3)</span>
<span class="c1">#middlewares下载中间件</span>
<span class="c1">#导入随机数模块，目的是随机挑选一个IP池中的ip</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="c1">#从settings文件（myfirstpjt.settings为settings文件的地址）中导入设置好的IPPOOL</span>
<span class="kn">from</span> <span class="nn">myfirstpjt.settings</span> <span class="kn">import</span> <span class="n">IPPOOL</span>
<span class="c1">#导入官方文档中HttpProxyMiddleware对应的模块</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.downloadermiddleware.httpproxy</span> <span class="kn">import</span> <span class="n">HttpProxyMiddleware</span>

<span class="k">class</span> <span class="nc">IPPOOLS</span><span class="p">(</span><span class="n">HttpProxyMiddleware</span><span class="p">):</span>
<span class="c1">#初始化方法</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">ip</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ip</span><span class="o">=</span><span class="n">ip</span>
<span class="c1">#process_request()方法，主要进行请求处理</span>
    <span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">request</span><span class="p">,</span><span class="n">spider</span><span class="p">):</span>
<span class="c1">#先随机选择一个IP</span>
        <span class="n">thisip</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">IPPOOL</span><span class="p">)</span>
<span class="c1">#输出当前选择的IP，便于调试观察</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;当前使用的IP是：&quot;</span><span class="o">+</span><span class="n">thisip</span><span class="p">[</span><span class="s2">&quot;ipaddr&quot;</span><span class="p">])</span>
<span class="c1">#将对应的IP实际添加为具体的代理，用该IP进行爬取</span>
        <span class="n">request</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;proxy&quot;</span><span class="p">]</span><span class="o">=</span><span class="s2">&quot;http://&quot;</span><span class="o">+</span><span class="n">thisip</span><span class="p">[</span><span class="s2">&quot;ipaddr&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>编写好下载中间件之后，此时在Scrapy项目中，middlewares.py文件只是一个普通的python程序，并不会默认为下载中间件。
如果想让middlewares.py文件作为项目的下载中间件文件，还需要在<code class="docutils literal notranslate"><span class="pre">setting.py</span></code>文件中，与下载中间件相关的配置信息默认如下：</p>
<p><code class="docutils literal notranslate"><span class="pre">setting.py</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#(4)</span>
<span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1">#&#39;myfirstpjt.middlewares.MyCustomDownloaderMiddleware&#39;: 543,</span>
    <span class="s1">&#39;scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware&#39;</span><span class="p">:</span><span class="mi">123</span><span class="p">,</span>
    <span class="s1">&#39;myfirstpjt.middlewares.IPPOOLS&#39;</span><span class="p">:</span><span class="mi">125</span>
<span class="p">}</span>
</pre></div>
</div>
<p>随后我们可以运行该Scrapy项目下的爬虫文件了。
<code class="docutils literal notranslate"><span class="pre">Scrapy</span> <span class="pre">crawl</span> <span class="pre">weisuen</span> <span class="pre">--nolog</span></code></p>
</section>
<section id="id9">
<h2><a class="toc-backref" href="#id23">4.使用用户代理池</a><a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>网站服务器可以识别爬行时候的用户代理（User-Agent）信息，通过用户信息可以判断出我们使用的是什么浏览器、什么形式的客户端。</p>
<p>作为爬虫方，显然不希望这样被封禁，为了避免这一类的禁止，我们可以使用用户代理池进行处理。</p>
<p>这一种处理方式与IP代理池的处理方式类似，我们可以收集多种浏览器的信息，以此建立一个用户代理池，然后建立一个下载中间件，
在下载中间件中设置每次随机认识用户代理池中的一个用户代理进行爬行。</p>
<p>IP代理池中使用的下载中间件类型为<code class="docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></p>
<p>用户代理池中使用的下载中间件类型为<code class="docutils literal notranslate"><span class="pre">UserAgentMiddleware</span></code></p>
<p>①需要在<code class="docutils literal notranslate"><span class="pre">setting.py</span></code>配置文件中设置好用户代理池。代理池名称可以自定义，如下所示：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#用户代理（user-agent）池设置</span>
<span class="n">UAPOOL</span><span class="o">=</span><span class="p">[</span>
    <span class="s2">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:48.0) Gecko/20100101 Firefox/48.0&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.5&quot;</span>
<span class="p">]</span>
</pre></div>
</div>
<p>②创建一个下载中间件文件<code class="docutils literal notranslate"><span class="pre">uamid.py</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#uamid下载中间件</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">myfirstpjt.settings</span> <span class="kn">import</span> <span class="n">UAPOOL</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.downloadermiddleware.useragent</span> <span class="kn">import</span> <span class="n">UserAgentMiddleware</span>

<span class="k">class</span> <span class="nc">Uamid</span><span class="p">(</span><span class="n">UserAgentMiddleware</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">ua</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ua</span><span class="o">=</span><span class="n">ua</span>
    <span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">request</span><span class="p">,</span><span class="n">spider</span><span class="p">):</span>
        <span class="n">thisua</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">UAPOOL</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;当前使用的user-agent是：&quot;</span><span class="o">+</span><span class="n">thisua</span><span class="p">)</span>
        <span class="n">request</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">&#39;User-Agent&#39;</span><span class="p">,</span><span class="n">thisua</span><span class="p">)</span>
</pre></div>
</div>
<p>③
需要在<code class="docutils literal notranslate"><span class="pre">setting.py</span></code>文件中将该Python文件设置为Scrapy中的下载中间件。打开<code class="docutils literal notranslate"><span class="pre">setting.py</span></code>文件，找到有关下载中间件设置的地方
(DOWNLOADER_MIDDLEWARES),修改为如下代码：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enable or disable downloader middlewares</span>
<span class="c1"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span>
<span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1">#&#39;myfirstpjt.middlewares.MyCustomDownloaderMiddleware&#39;: 543,</span>
    <span class="s1">&#39;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&#39;</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;myfirstpjt.uamid.Uamid&#39;</span><span class="p">:</span><span class="mi">1</span>
<span class="p">}</span>
</pre></div>
</div>
<p>随后我们可以运行该Scrapy项目下的爬虫文件了。
<code class="docutils literal notranslate"><span class="pre">Scrapy</span> <span class="pre">crawl</span> <span class="pre">weisuen</span> <span class="pre">--nolog</span></code></p>
<section id="id10">
<h3><a class="toc-backref" href="#id24">注意！：</a><a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>如果要对爬取的网站进行爬取，默认遵守<code class="docutils literal notranslate"><span class="pre">robots.txt</span></code>爬虫协议,需要爬取，则让爬虫项目不遵循<code class="docutils literal notranslate"><span class="pre">robots.txt</span></code>规则即可。</p>
<p>打开爬虫项目下的<code class="docutils literal notranslate"><span class="pre">setting.py</span></code>文件，找到ROBOTSTXT部分的设置，默认设置如下所示：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obey robots.txt rules</span>
<span class="n">ROBOTSTXT_OBEY</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>我们可以修改为：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obey robots.txt rules</span>
<span class="n">ROBOTSTXT_OBEY</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="02.MongoDB%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E4%B8%8E%E5%AF%BC%E5%87%BA.html" class="btn btn-neutral float-left" title="23.3.2. MongoDB的常用操作与导出" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="04.Scrapy%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%92%8C%E5%9B%BE%E7%89%87.html" class="btn btn-neutral float-right" title="23.3.4. Scrapy下载文件和图片" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, huxiaojian.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>